# -*- coding: utf-8 -*-
"""ANN Binary and MultiClass Classification - Tensorflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1npWRwANVd7FjXlm8wYx_wqiF3vIWZMN5

Using TensorFlow/Keras
"""

import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Filter for digits 7 and 9 (since my SJSU ID is 017557579)
train_mask = np.where((y_train == 7) | (y_train == 9))[0]
test_mask = np.where((y_test == 7) | (y_test == 9))[0]

print(train_mask)

#flitering 7s and 9s in both train and test datasets
X_train = x_train[train_mask]
y_train = y_train[train_mask]
X_test = x_test[test_mask]
y_test = y_test[test_mask]

print (X_train,X_test)

# convert 9 as 1 and 7 as 0
y_train = (y_train == 9).astype(int)
y_test = (y_test == 9).astype(int)

# Normalize values
#image has pixel values from 0 to 255. so dividing all values by 255 will make values between 0 and 1
X_train = X_train / 255.0
X_test = X_test / 255.0

# Split train data into train and validation sets
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Basic Binary Classification Model
def create_base_model():
    model = Sequential([
        Flatten(input_shape=(28, 28)),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam',
                 loss='binary_crossentropy',
                 metrics=['accuracy'])
    return model

##explaination of each line for future reference
# Flatten: Turns 28x28 image into a long line of numbers
# Dense(128): First hidden layer with 128 neurons
# Dense(64): Second hidden layer with 64 neurons
# Dropout: Randomly turns off some neurons to prevent overfitting
# Dense(1): Output layer - gives us 0 (7) or 1 (9)

# optimizer='adam': Tells the model how to learn
# loss='binary_crossentropy': How we measure mistakes
# metrics=['accuracy']: Evalution metric

# Early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

# If model stops improving for 3 epochs the stop training

# Train base model
base_model = create_base_model()
history = base_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    callbacks=[early_stopping],
    verbose=1
)

# Evaluate base model
y_pred = (base_model.predict(X_test) > 0.5).astype(int)
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label (0=7, 1=9)')
plt.xlabel('Predicted Label (0=7, 1=9)')
plt.show()

# Plot learning curves
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss - Base Model')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# 3. Show Example Predictions
def plot_predictions(X, y_true, y_pred, num_examples=5):
    indices = np.random.randint(0, len(X), num_examples)

    plt.figure(figsize=(15, 3))
    for i, idx in enumerate(indices):
        plt.subplot(1, num_examples, i+1)
        plt.imshow(X[idx], cmap='gray')
        plt.axis('off')
        true_digit = '7' if y_true[idx] == 0 else '9'
        pred_digit = '7' if y_pred[idx] == 0 else '9'
        color = 'green' if y_true[idx] == y_pred[idx] else 'red'
        plt.title(f'True: {true_digit}\nPred: {pred_digit}', color=color)
    plt.tight_layout()
    plt.show()

# Show example predictions
plot_predictions(X_test, y_test, y_pred)

# Print final metrics
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {test_accuracy:.4f}")
print("\nConfusion Matrix Interpretation:")
print(f"True Negatives (Correctly identified 7s): {conf_matrix[0,0]}")
print(f"False Positives (7s predicted as 9s): {conf_matrix[0,1]}")
print(f"False Negatives (9s predicted as 7s): {conf_matrix[1,0]}")
print(f"True Positives (Correctly identified 9s): {conf_matrix[1,1]}")

# Part B: Different combinations of initializers and activation functions
# creating a helper function for trying different combinations of initializers and activation functions
def create_model(initializer, activation):
    model = Sequential([
        Flatten(input_shape=(28, 28)),
        Dense(128, kernel_initializer=initializer, activation=activation),
        Dropout(0.3),
        Dense(64, kernel_initializer=initializer, activation=activation),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam',
                 loss='binary_crossentropy',
                 metrics=['accuracy'])
    return model

# Define combinations
initializers = {
    'normal': 'random_normal',
    'he': 'he_normal',
    'xavier': 'glorot_normal'
}

activations = ['relu', 'sigmoid', 'tanh']

# Store results
results = []

import pandas as pd
# Train and evaluate different combinations
for init_name, init in initializers.items():
    for act in activations:
        print(f"\nTraining model with {init_name} initializer and {act} activation")

        model = create_model(init, act)
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=100,
            callbacks=[early_stopping], #calling early stopping function
            verbose=0
        )

        # Evaluate
        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
        results.append({
            'initializer': init_name,
            'activation': act,
            'accuracy': test_acc,
            'epochs_trained': len(history.history['loss'])
        })

        # Plot learning curves
        plt.figure(figsize=(10, 5))
        plt.plot(history.history['loss'], label='Training Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title(f'Model Loss - {init_name} initializer, {act} activation')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.show()

        # Confusion matrix
        y_pred = (model.predict(X_test) > 0.5).astype(int)
        conf_matrix = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
        plt.title(f'Confusion Matrix - {init_name} initializer, {act} activation')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.show()

# Display results table
results_df = pd.DataFrame(results)
print("\nResults Summary:")
print(results_df)

"""**Accuracy Performance:**


He initializer with ReLU activation achieved the highest accuracy (0.993127)

Normal initializer with ReLU also performed very well (0.991654)

Sigmoid and tanh activations  showed slightly lower accuracies compared to ReLU


**Training Efficiency (Epochs):**


ReLU activations generally required fewer epochs to converge (9-14 epochs)

Sigmoid activation needed more training time (18-27 epochs)

He initializer with ReLU was efficient, reaching high accuracy in just 14 epochs

Xavier with sigmoid took the longest at 27 epochs


**Learning Curve Analysis:**


**ReLU combinations:**

Faster initial convergence

More stable validation loss

Smaller gap between training and validation loss, suggesting good generalization



**Sigmoid combinations:**

Slower convergence, More gradual learning curve

Slightly higher final validation loss

For sigmoid also there is a small gap between training and validation loss


**Tanh combinations:**

Moderate convergence speed

Some evidence of overfitting (validation loss higher than training loss)




**Initializer Impact:**


**He initializer:**

Works well with ReLU
Provides consistent performance across activations
Shows better stability in validation loss


**Xavier initializer:**

Good performance with all activation functions
Effective with sigmoid
Shows slightly more variance in validation loss


**Normal initializer:**

Competitive performance but slightly less consistent
Shows more gap between training and validation loss

# **ANN for multi-class classification**
"""

import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load MNIST data
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize pixel values
X_train = x_train / 255.0
X_test = x_test / 255.0

# Convert labels to categorical
y_train_cat = to_categorical(y_train)
y_test_cat = to_categorical(y_test)

## for future refernce
## we are using to categprical here to convert numbers into one-hot encoded vectors
## One-Hot Encoding - convert numbers into vectors

## Each digit is represented by a vector of length 10
## A '1' is placed in the position corresponding to the digit, with '0's everywhere else

# Split training data into train and validation sets
from sklearn.model_selection import train_test_split
X_train, X_val, y_train_cat, y_val_cat = train_test_split(
    X_train, y_train_cat, test_size=0.2, random_state=42
)

# Create the model
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(256, activation='relu', kernel_initializer='he_normal'),
    Dropout(0.3),
    Dense(128, activation='relu', kernel_initializer='he_normal'),
    Dropout(0.2),
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

# Train the model
history = model.fit(
    X_train, y_train_cat,
    epochs=100,
    batch_size=128,
    validation_data=(X_val, y_val_cat),
    callbacks=[early_stopping]
)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test_cat)

# Get predictions
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)

# Generate classification report
report = classification_report(y_test, y_pred, digits=4)
print("\nClassification Report:")
print(report)

# Create confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# Plot learning curves
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Learning Curves - Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Learning Curves - Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

# Display some example predictions
num_examples = 10
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
axes = axes.ravel()

random_indices = np.random.randint(0, len(X_test), num_examples)
for idx, ax in enumerate(axes):
    img_idx = random_indices[idx]
    ax.imshow(X_test[img_idx], cmap='gray')
    ax.set_title(f'True: {y_test[img_idx]}\nPred: {y_pred[img_idx]}')
    ax.axis('off')
plt.tight_layout()
plt.show()



"""Overall accuracy of the model is 97.98%

The model performs well across all digit classes.


Class-wise Analysis:

1. Digit 1 shows the highest metrics (precision: 0.9912, recall: 0.9903, F1: 0.9907)
I think this is because '1' has a simple, distinctive shape that is easy to recognize.

2. Digit 0 performs very well (precision: 0.9868, recall: 0.9888, F1: 0.9878)
3. Digits 6 and 5 also show strong performance with F1-scores above 0.984.

4. Digit 7 and 9 have slightly lower F1-scores (0.9734 and 0.9736 respectively). I think this is because 7 and 9 are relatively similar in shape.
5. Digit 8 also shows relatively lower metrics (precision: 0.9763, recall: 0.9743)

Support Distribution:


1. The dataset is reasonably balanced
2. Sample sizes range from 892 (digit 5) to 1135 (digit 1)
Balanced distribution mean reliable performance metrics


Interpretation:


High precision across all classes -> few false positives

Strong recall values -> model rarely misses actual instances

Close values between precision and recall -> balanced performance

Small variance in F1-scores (range: 0.9734-0.9907) -> consistent performance across all digits
"""